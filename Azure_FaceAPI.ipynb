{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Azure_FaceAPI.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3b2yt9LILc4pemoUHavLZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariatomy9/Azure-Face-API/blob/main/Azure_FaceAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRzZRo0zdT1f"
      },
      "source": [
        "# Face detection from URL Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC6ixRBRBP3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c170d4-2fab-40c7-f907-3a2e673d34a8"
      },
      "source": [
        "!pip install --upgrade azure-cognitiveservices-vision-face"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting azure-cognitiveservices-vision-face\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/94/1916bebc3e70d4ad3108c43ee366d61e0ba8784c33853eb8cfa19a6ef775/azure_cognitiveservices_vision_face-0.5.0-py2.py3-none-any.whl (66kB)\n",
            "\r\u001b[K     |█████                           | 10kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 20kB 17.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 30kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.8MB/s \n",
            "\u001b[?25hCollecting azure-common~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/1e/cfe7987493242e8b9ead309cfd76fc500c38bbefe192192b813325d289f3/azure_common-1.1.27-py2.py3-none-any.whl\n",
            "Collecting msrest>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/cc/6c96bfb3d3cf4c3bdedfa6b46503223f4c2a4fa388377697e0f8082a4fed/msrest-0.6.21-py2.py3-none-any.whl (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2021.5.30)\n",
            "Collecting isodate>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: requests~=2.16 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from isodate>=0.6.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.10)\n",
            "Installing collected packages: azure-common, isodate, msrest, azure-cognitiveservices-vision-face\n",
            "Successfully installed azure-cognitiveservices-vision-face-0.5.0 azure-common-1.1.27 isodate-0.6.0 msrest-0.6.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugA9d7XRBP6g"
      },
      "source": [
        "import asyncio\n",
        "import io\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import uuid\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "from io import BytesIO\n",
        "# To install this module, run:\n",
        "# python -m pip install Pillow\n",
        "from PIL import Image, ImageDraw\n",
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from azure.cognitiveservices.vision.face.models import FaceAttributeType, HairColorType, TrainingStatusType, Person"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7JLcMGoBP8O"
      },
      "source": [
        "# This key will serve all examples in this document.\n",
        "KEY = \"Please enter azure key\"\n",
        "\n",
        "# This endpoint will be used in all examples in this quickstart.\n",
        "ENDPOINT = \"Please enter azure endpoint\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt3eINlgBP-9"
      },
      "source": [
        "# Create an authenticated FaceClient.\n",
        "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUpTDdaNBQBW",
        "outputId": "fe2bd8f9-1456-4886-a91e-cd1520ffedc5"
      },
      "source": [
        "# Detect a face in an image that contains a single face\n",
        "single_face_image_url = 'https://upload.wikimedia.org/wikipedia/commons/5/55/Dalailama1_20121014_4639.jpg'\n",
        "single_image_name = os.path.basename(single_face_image_url)\n",
        "# We use detection model 3 to get better performance.\n",
        "detected_faces = face_client.face.detect_with_url(url=single_face_image_url, detection_model='detection_03')\n",
        "if not detected_faces:\n",
        "    raise Exception('No face detected from image {}'.format(single_image_name))\n",
        "\n",
        "# Display the detected face ID in the first single-face image.\n",
        "# Face IDs are used for comparison to faces (their IDs) detected in other images.\n",
        "print('Detected face ID from', single_image_name, ':')\n",
        "for face in detected_faces: print (face.face_id)\n",
        "print()\n",
        "\n",
        "# Save this ID for use in Find Similar\n",
        "first_image_face_ID = detected_faces[0].face_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected face ID from Dalailama1_20121014_4639.jpg :\n",
            "9d9e6fb6-951b-43cc-bf0e-310627518aaa\n",
            "b6f5e253-c5b4-46c1-a1fc-558737dfb4c8\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvVeGjgAbhDy",
        "outputId": "3e78387d-07c4-4ab5-a08e-b43d78aa8d58"
      },
      "source": [
        "# Detect a face in an image that contains a single face\n",
        "single_face_image_url = 'https://upload.wikimedia.org/wikipedia/commons/5/55/Dalailama1_20121014_4639.jpg'\n",
        "single_image_name = os.path.basename(single_face_image_url)\n",
        "# We use detection model 3 to get better performance.\n",
        "detected_faces = face_client.face.detect_with_url(url=single_face_image_url, detection_model='detection_03')\n",
        "if not detected_faces:\n",
        "    raise Exception('No face detected from image {}'.format(single_image_name))\n",
        "\n",
        "# Convert width height to a point in a rectangle\n",
        "def getRectangle(faceDictionary):\n",
        "    rect = faceDictionary.face_rectangle\n",
        "    left = rect.left\n",
        "    top = rect.top\n",
        "    right = left + rect.width\n",
        "    bottom = top + rect.height\n",
        "    \n",
        "    return ((left, top), (right, bottom))\n",
        "\n",
        "def drawFaceRectangles() :\n",
        "# Download the image from the url\n",
        "    response = requests.get(single_face_image_url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# For each face returned use the face rectangle and draw a red box.\n",
        "    print('Drawing rectangle around face... see popup for results.')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for face in detected_faces:\n",
        "        draw.rectangle(getRectangle(face), outline='blue')\n",
        "\n",
        "# Display the image in the default image browser.\n",
        "    #img.show()\n",
        "    img=img.save('Face.jpeg')\n",
        "# Uncomment this to show the face rectangles.\n",
        "drawFaceRectangles()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drawing rectangle around face... see popup for results.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHk5vwUjdO2o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyVCNk_CdO75"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fkte5K3c_rK"
      },
      "source": [
        "# Extracting images from video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LFV5EOObhQw"
      },
      "source": [
        "# Importing all necessary libraries\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Read the video from specified path\n",
        "cam = cv2.VideoCapture(\"police.mp4\")\n",
        "\n",
        "try:\n",
        "\t\n",
        "\t# creating a folder named data\n",
        "\tif not os.path.exists('data'):\n",
        "\t\tos.makedirs('data')\n",
        "\n",
        "# if not created then raise error\n",
        "except OSError:\n",
        "\tprint ('Error: Creating directory of data')\n",
        "\n",
        "# frame\n",
        "currentframe = 0\n",
        "\n",
        "while(True):\n",
        "\t\n",
        "\t# reading from frame\n",
        "\tret,frame = cam.read()\n",
        "\n",
        "\tif ret:\n",
        "\t\t# if video is still left continue creating images\n",
        "\t\tname = './data/frame' + str(currentframe) + '.jpg'\n",
        "\t\tprint ('Creating...' + name)\n",
        "\n",
        "\t\t# writing the extracted images\n",
        "\t\tcv2.imwrite(name, frame)\n",
        "\n",
        "\t\t# increasing counter so that it will\n",
        "\t\t# show how many frames are created\n",
        "\t\tcurrentframe += 1\n",
        "\telse:\n",
        "\t\tbreak\n",
        "\n",
        "# Release all space and windows once done\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAb3B5nsqGVs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5dBYrNiqGSx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-e4zOvMp0Rd"
      },
      "source": [
        "# Features from face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbR3hcW6lc2W",
        "outputId": "74995ccc-70ca-4587-9b90-1d9e112755f4"
      },
      "source": [
        "from io import BytesIO\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "\n",
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from azure.cognitiveservices.vision.face.models import FaceAttributeType\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "# This key will serve all examples in this document.\n",
        "KEY = ''\n",
        "\n",
        "# This endpoint will be used in all examples in this quickstart.\n",
        "ENDPOINT = ''\n",
        "\n",
        "# Create an authenticated FaceClient.\n",
        "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))\n",
        "\n",
        "# Image of face(s)\n",
        "face1_url = 'https://upload.wikimedia.org/wikipedia/commons/5/55/Dalailama1_20121014_4639.jpg'\n",
        "face1_name = os.path.basename(face1_url)\n",
        "# face2_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/The_famous_mustache_and_goatee.jpg/220px-The_famous_mustache_and_goatee.jpg'\n",
        "# face2_name = os.path.basename(face2_url)\n",
        "\n",
        "# List of url images\n",
        "url_images = [face1_url]\n",
        "\n",
        "# Attributes you want returned with the API call, a list of FaceAttributeType enum (string format)\n",
        "face_attributes = ['age', 'gender', 'headPose', 'smile', 'facialHair', 'glasses', 'emotion']\n",
        "\n",
        "# Detect a face with attributes, returns a list[DetectedFace]\n",
        "for image in url_images:\n",
        "    detected_faces = face_client.face.detect_with_url(url=image, return_face_attributes=face_attributes)\n",
        "    if not detected_faces:\n",
        "        raise Exception(\n",
        "            'No face detected from image {}'.format(os.path.basename(image)))\n",
        "\n",
        "    '''\n",
        "    Display the detected face with attributes and bounding box\n",
        "    '''\n",
        "    # Face IDs are used for comparison to faces (their IDs) detected in other images.\n",
        "    for face in detected_faces:\n",
        "        print()\n",
        "        print('Detected face ID from', os.path.basename(image), ':')\n",
        "        # ID of detected face\n",
        "        print(face.face_id)\n",
        "        # Show all facial attributes from the results\n",
        "        print()\n",
        "        print('Facial attributes detected:')\n",
        "        print('Age: ', face.face_attributes.age)\n",
        "        print('Gender: ', face.face_attributes.gender)\n",
        "        print('Head pose: ', face.face_attributes.head_pose)\n",
        "        print('Smile: ', face.face_attributes.smile)\n",
        "        print('Facial hair: ', face.face_attributes.facial_hair)\n",
        "        print('Glasses: ', face.face_attributes.glasses)\n",
        "        print('Emotion: ')\n",
        "        print('\\tAnger: ', face.face_attributes.emotion.anger)\n",
        "        print('\\tContempt: ', face.face_attributes.emotion.contempt)\n",
        "        print('\\tDisgust: ', face.face_attributes.emotion.disgust)\n",
        "        print('\\tFear: ', face.face_attributes.emotion.fear)\n",
        "        print('\\tHappiness: ', face.face_attributes.emotion.happiness)\n",
        "        print('\\tNeutral: ', face.face_attributes.emotion.neutral)\n",
        "        print('\\tSadness: ', face.face_attributes.emotion.sadness)\n",
        "        print('\\tSurprise: ', face.face_attributes.emotion.surprise)\n",
        "        print()\n",
        "\n",
        "    # Convert width height to a point in a rectangle\n",
        "    def getRectangle(faceDictionary):\n",
        "        rect = faceDictionary.face_rectangle\n",
        "        left = rect.left\n",
        "        top = rect.top\n",
        "        right = left + rect.width\n",
        "        bottom = top + rect.height\n",
        "\n",
        "        return ((left, top), (right, bottom))\n",
        "\n",
        "    # Download the image from the url, so can display it in popup/browser\n",
        "    response = requests.get(image)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # For each face returned use the face rectangle and draw a blue box.\n",
        "    print('Drawing rectangle around face... see popup for results.')\n",
        "    print()\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for face in detected_faces:\n",
        "        draw.rectangle(getRectangle(face), outline='blue')\n",
        "\n",
        "    # Display the image in the users default image browser.\n",
        "    img=img.save('Face.jpeg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Detected face ID from Dalailama1_20121014_4639.jpg :\n",
            "cf29e7c2-3762-4d53-b2ba-fa69ddcad00e\n",
            "\n",
            "Facial attributes detected:\n",
            "Age:  65.0\n",
            "Gender:  Gender.male\n",
            "Head pose:  {'additional_properties': {}, 'roll': -6.4, 'yaw': -4.6, 'pitch': -12.5}\n",
            "Smile:  0.993\n",
            "Facial hair:  {'additional_properties': {}, 'moustache': 0.1, 'beard': 0.1, 'sideburns': 0.1}\n",
            "Glasses:  GlassesType.no_glasses\n",
            "Emotion: \n",
            "\tAnger:  0.0\n",
            "\tContempt:  0.005\n",
            "\tDisgust:  0.0\n",
            "\tFear:  0.0\n",
            "\tHappiness:  0.993\n",
            "\tNeutral:  0.002\n",
            "\tSadness:  0.0\n",
            "\tSurprise:  0.0\n",
            "\n",
            "Drawing rectangle around face... see popup for results.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrYeDeG0qBIA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCOlC_I7qBEj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QhcAgmPp8Mx"
      },
      "source": [
        "# Features(Emotions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkp4XWj_lc5y",
        "outputId": "9992e317-8e9a-430f-96ec-f72b0d3af37a"
      },
      "source": [
        "from io import BytesIO\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "\n",
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from azure.cognitiveservices.vision.face.models import FaceAttributeType\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "# This key will serve all examples in this document.\n",
        "KEY = ''\n",
        "\n",
        "# This endpoint will be used in all examples in this quickstart.\n",
        "ENDPOINT = ''\n",
        "\n",
        "# Create an authenticated FaceClient.\n",
        "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))\n",
        "\n",
        "# Image of face(s)\n",
        "face1_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/President_Trump_Delivers_Remarks_%2849498772251%29.jpg/1280px-President_Trump_Delivers_Remarks_%2849498772251%29.jpg'\n",
        "face1_name = os.path.basename(face1_url)\n",
        "# face2_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/The_famous_mustache_and_goatee.jpg/220px-The_famous_mustache_and_goatee.jpg'\n",
        "# face2_name = os.path.basename(face2_url)\n",
        "\n",
        "# List of url images\n",
        "url_images = [face1_url]\n",
        "\n",
        "# Attributes you want returned with the API call, a list of FaceAttributeType enum (string format)\n",
        "face_attributes = ['age', 'gender', 'glasses', 'emotion']\n",
        "\n",
        "# Detect a face with attributes, returns a list[DetectedFace]\n",
        "for image in url_images:\n",
        "    detected_faces = face_client.face.detect_with_url(url=image, return_face_attributes=face_attributes)\n",
        "    if not detected_faces:\n",
        "        raise Exception(\n",
        "            'No face detected from image {}'.format(os.path.basename(image)))\n",
        "\n",
        "    '''\n",
        "    Display the detected face with attributes and bounding box\n",
        "    '''\n",
        "    # Face IDs are used for comparison to faces (their IDs) detected in other images.\n",
        "    for face in detected_faces:\n",
        "        print()\n",
        "        print('Detected face ID from', os.path.basename(image), ':')\n",
        "        # ID of detected face\n",
        "        print(face.face_id)\n",
        "        # Show all facial attributes from the results\n",
        "        print()\n",
        "        print('Facial attributes detected:')\n",
        "        print('Age: ', face.face_attributes.age)\n",
        "        print('Gender: ', face.face_attributes.gender)\n",
        "        print('Glasses: ', face.face_attributes.glasses)\n",
        "        print('Emotion: ')\n",
        "        print('\\tAnger: ', face.face_attributes.emotion.anger)\n",
        "        print('\\tContempt: ', face.face_attributes.emotion.contempt)\n",
        "        print('\\tDisgust: ', face.face_attributes.emotion.disgust)\n",
        "        print('\\tFear: ', face.face_attributes.emotion.fear)\n",
        "        print('\\tHappiness: ', face.face_attributes.emotion.happiness)\n",
        "        print('\\tNeutral: ', face.face_attributes.emotion.neutral)\n",
        "        print('\\tSadness: ', face.face_attributes.emotion.sadness)\n",
        "        print('\\tSurprise: ', face.face_attributes.emotion.surprise)\n",
        "        print()\n",
        "\n",
        "    # Convert width height to a point in a rectangle\n",
        "    def getRectangle(faceDictionary):\n",
        "        rect = faceDictionary.face_rectangle\n",
        "        left = rect.left\n",
        "        top = rect.top\n",
        "        right = left + rect.width\n",
        "        bottom = top + rect.height\n",
        "\n",
        "        return ((left, top), (right, bottom))\n",
        "\n",
        "    # Download the image from the url, so can display it in popup/browser\n",
        "    response = requests.get(image)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # For each face returned use the face rectangle and draw a blue box.\n",
        "    print('Drawing rectangle around face... see popup for results.')\n",
        "    print()\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for face in detected_faces:\n",
        "        draw.rectangle(getRectangle(face), outline='blue')\n",
        "\n",
        "    # Display the image in the users default image browser.\n",
        "    img=img.save('Face1.jpeg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Detected face ID from 1280px-President_Trump_Delivers_Remarks_%2849498772251%29.jpg :\n",
            "94d24dc4-580f-4a96-8233-d15c35950aac\n",
            "\n",
            "Facial attributes detected:\n",
            "Age:  60.0\n",
            "Gender:  Gender.male\n",
            "Glasses:  GlassesType.no_glasses\n",
            "Emotion: \n",
            "\tAnger:  0.0\n",
            "\tContempt:  0.004\n",
            "\tDisgust:  0.0\n",
            "\tFear:  0.0\n",
            "\tHappiness:  0.007\n",
            "\tNeutral:  0.828\n",
            "\tSadness:  0.161\n",
            "\tSurprise:  0.0\n",
            "\n",
            "Drawing rectangle around face... see popup for results.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMjqzKxTqeJZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH7RJEfzqeM8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsrfI0pvqYJI"
      },
      "source": [
        "# Generating Json text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmZh_dS5o5cq",
        "outputId": "0bf08326-e74e-4f00-e009-5e7e6ff31801"
      },
      "source": [
        "# changed detection model from 3 to 1\n",
        "import json, os, requests\n",
        "\n",
        "subscription_key = \"\"\n",
        "\n",
        "face_api_url = \"\" + '/face/v1.0/detect'\n",
        "\n",
        "image_url = 'https://upload.wikimedia.org/wikipedia/commons/5/55/Dalailama1_20121014_4639.jpg'\n",
        "\n",
        "headers = {'Ocp-Apim-Subscription-Key': subscription_key}\n",
        "\n",
        "params = {\n",
        "    'detectionModel': 'detection_01',\n",
        "    'returnFaceId': 'true',\n",
        "    'returnFaceAttributes': 'age,gender,headPose,smile,facialHair,glasses,emotion,hair,makeup,occlusion,accessories,blur,exposure,noise'\n",
        "}\n",
        "\n",
        "response = requests.post(face_api_url, params=params,\n",
        "                         headers=headers, json={\"url\": image_url})\n",
        "print(json.dumps(response.json()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{\"faceId\": \"046bd466-0c30-4970-b322-0e74bbc40186\", \"faceRectangle\": {\"top\": 292, \"left\": 321, \"width\": 356, \"height\": 356}, \"faceAttributes\": {\"smile\": 0.993, \"headPose\": {\"pitch\": -12.5, \"roll\": -6.4, \"yaw\": -4.6}, \"gender\": \"male\", \"age\": 65.0, \"facialHair\": {\"moustache\": 0.1, \"beard\": 0.1, \"sideburns\": 0.1}, \"glasses\": \"NoGlasses\", \"emotion\": {\"anger\": 0.0, \"contempt\": 0.005, \"disgust\": 0.0, \"fear\": 0.0, \"happiness\": 0.993, \"neutral\": 0.002, \"sadness\": 0.0, \"surprise\": 0.0}, \"blur\": {\"blurLevel\": \"low\", \"value\": 0.0}, \"exposure\": {\"exposureLevel\": \"goodExposure\", \"value\": 0.68}, \"noise\": {\"noiseLevel\": \"low\", \"value\": 0.0}, \"makeup\": {\"eyeMakeup\": true, \"lipMakeup\": false}, \"accessories\": [], \"occlusion\": {\"foreheadOccluded\": false, \"eyeOccluded\": false, \"mouthOccluded\": false}, \"hair\": {\"bald\": 0.98, \"invisible\": false, \"hairColor\": []}}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "dDFJEBXco5fT",
        "outputId": "2684c266-e06f-4087-9218-45f8ec80932f"
      },
      "source": [
        "response.text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[{\"faceId\":\"046bd466-0c30-4970-b322-0e74bbc40186\",\"faceRectangle\":{\"top\":292,\"left\":321,\"width\":356,\"height\":356},\"faceAttributes\":{\"smile\":0.993,\"headPose\":{\"pitch\":-12.5,\"roll\":-6.4,\"yaw\":-4.6},\"gender\":\"male\",\"age\":65.0,\"facialHair\":{\"moustache\":0.1,\"beard\":0.1,\"sideburns\":0.1},\"glasses\":\"NoGlasses\",\"emotion\":{\"anger\":0.0,\"contempt\":0.005,\"disgust\":0.0,\"fear\":0.0,\"happiness\":0.993,\"neutral\":0.002,\"sadness\":0.0,\"surprise\":0.0},\"blur\":{\"blurLevel\":\"low\",\"value\":0.0},\"exposure\":{\"exposureLevel\":\"goodExposure\",\"value\":0.68},\"noise\":{\"noiseLevel\":\"low\",\"value\":0.0},\"makeup\":{\"eyeMakeup\":true,\"lipMakeup\":false},\"accessories\":[],\"occlusion\":{\"foreheadOccluded\":false,\"eyeOccluded\":false,\"mouthOccluded\":false},\"hair\":{\"bald\":0.98,\"invisible\":false,\"hairColor\":[]}}}]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6lKoJJLo5hb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0SVqkkqo5kx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}