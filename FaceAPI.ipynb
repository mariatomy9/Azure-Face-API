{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceAPI.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPD9Nxlxre28h2UvCHj7Ke/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariatomy9/Azure-Face-API/blob/main/FaceAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRzZRo0zdT1f"
      },
      "source": [
        "# Face detection from URL Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmYY7TzMA94m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ad840a6-0f63-4503-b4f2-3202f1a5d775"
      },
      "source": [
        "\"\"\"\n",
        "API : 8484b232906249b3851bda2a5b49b70e\n",
        "Endpoint : https://newface2021.cognitiveservices.azure.com/\n",
        "Location : eastus\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAPI : 8484b232906249b3851bda2a5b49b70e\\nEndpoint : https://newface2021.cognitiveservices.azure.com/\\nLocation : eastus\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC6ixRBRBP3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44a933b-6d87-40ae-afae-2206f9df4feb"
      },
      "source": [
        "!pip install --upgrade azure-cognitiveservices-vision-face"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting azure-cognitiveservices-vision-face\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/94/1916bebc3e70d4ad3108c43ee366d61e0ba8784c33853eb8cfa19a6ef775/azure_cognitiveservices_vision_face-0.5.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hCollecting msrest>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/cc/6c96bfb3d3cf4c3bdedfa6b46503223f4c2a4fa388377697e0f8082a4fed/msrest-0.6.21-py2.py3-none-any.whl (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.0MB/s \n",
            "\u001b[?25hCollecting azure-common~=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/1e/cfe7987493242e8b9ead309cfd76fc500c38bbefe192192b813325d289f3/azure_common-1.1.27-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: requests~=2.16 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.23.0)\n",
            "Collecting isodate>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from isodate>=0.6.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.15.0)\n",
            "Installing collected packages: isodate, msrest, azure-common, azure-cognitiveservices-vision-face\n",
            "Successfully installed azure-cognitiveservices-vision-face-0.5.0 azure-common-1.1.27 isodate-0.6.0 msrest-0.6.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugA9d7XRBP6g"
      },
      "source": [
        "import asyncio\n",
        "import io\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import uuid\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "from io import BytesIO\n",
        "# To install this module, run:\n",
        "# python -m pip install Pillow\n",
        "from PIL import Image, ImageDraw\n",
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from azure.cognitiveservices.vision.face.models import FaceAttributeType, HairColorType, TrainingStatusType, Person"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7JLcMGoBP8O"
      },
      "source": [
        "# This key will serve all examples in this document.\n",
        "KEY = \"8484b232906249b3851bda2a5b49b70e\"\n",
        "\n",
        "# This endpoint will be used in all examples in this quickstart.\n",
        "ENDPOINT = \"https://newface2021.cognitiveservices.azure.com/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt3eINlgBP-9"
      },
      "source": [
        "# Create an authenticated FaceClient.\n",
        "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUpTDdaNBQBW",
        "outputId": "fe2bd8f9-1456-4886-a91e-cd1520ffedc5"
      },
      "source": [
        "# Detect a face in an image that contains a single face\n",
        "single_face_image_url = 'https://upload.wikimedia.org/wikipedia/commons/5/55/Dalailama1_20121014_4639.jpg'\n",
        "single_image_name = os.path.basename(single_face_image_url)\n",
        "# We use detection model 3 to get better performance.\n",
        "detected_faces = face_client.face.detect_with_url(url=single_face_image_url, detection_model='detection_03')\n",
        "if not detected_faces:\n",
        "    raise Exception('No face detected from image {}'.format(single_image_name))\n",
        "\n",
        "# Display the detected face ID in the first single-face image.\n",
        "# Face IDs are used for comparison to faces (their IDs) detected in other images.\n",
        "print('Detected face ID from', single_image_name, ':')\n",
        "for face in detected_faces: print (face.face_id)\n",
        "print()\n",
        "\n",
        "# Save this ID for use in Find Similar\n",
        "first_image_face_ID = detected_faces[0].face_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected face ID from Dalailama1_20121014_4639.jpg :\n",
            "9d9e6fb6-951b-43cc-bf0e-310627518aaa\n",
            "b6f5e253-c5b4-46c1-a1fc-558737dfb4c8\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvVeGjgAbhDy",
        "outputId": "3e78387d-07c4-4ab5-a08e-b43d78aa8d58"
      },
      "source": [
        "# Detect a face in an image that contains a single face\n",
        "single_face_image_url = 'https://upload.wikimedia.org/wikipedia/commons/5/55/Dalailama1_20121014_4639.jpg'\n",
        "single_image_name = os.path.basename(single_face_image_url)\n",
        "# We use detection model 3 to get better performance.\n",
        "detected_faces = face_client.face.detect_with_url(url=single_face_image_url, detection_model='detection_03')\n",
        "if not detected_faces:\n",
        "    raise Exception('No face detected from image {}'.format(single_image_name))\n",
        "\n",
        "# Convert width height to a point in a rectangle\n",
        "def getRectangle(faceDictionary):\n",
        "    rect = faceDictionary.face_rectangle\n",
        "    left = rect.left\n",
        "    top = rect.top\n",
        "    right = left + rect.width\n",
        "    bottom = top + rect.height\n",
        "    \n",
        "    return ((left, top), (right, bottom))\n",
        "\n",
        "def drawFaceRectangles() :\n",
        "# Download the image from the url\n",
        "    response = requests.get(single_face_image_url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# For each face returned use the face rectangle and draw a red box.\n",
        "    print('Drawing rectangle around face... see popup for results.')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for face in detected_faces:\n",
        "        draw.rectangle(getRectangle(face), outline='blue')\n",
        "\n",
        "# Display the image in the default image browser.\n",
        "    #img.show()\n",
        "    img=img.save('Face.jpeg')\n",
        "# Uncomment this to show the face rectangles.\n",
        "drawFaceRectangles()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drawing rectangle around face... see popup for results.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHk5vwUjdO2o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Z3aaqIdO5G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyVCNk_CdO75"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fkte5K3c_rK"
      },
      "source": [
        "# Extracting images from video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LFV5EOObhQw"
      },
      "source": [
        "# Importing all necessary libraries\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Read the video from specified path\n",
        "cam = cv2.VideoCapture(\"police.mp4\")\n",
        "\n",
        "try:\n",
        "\t\n",
        "\t# creating a folder named data\n",
        "\tif not os.path.exists('data'):\n",
        "\t\tos.makedirs('data')\n",
        "\n",
        "# if not created then raise error\n",
        "except OSError:\n",
        "\tprint ('Error: Creating directory of data')\n",
        "\n",
        "# frame\n",
        "currentframe = 0\n",
        "\n",
        "while(True):\n",
        "\t\n",
        "\t# reading from frame\n",
        "\tret,frame = cam.read()\n",
        "\n",
        "\tif ret:\n",
        "\t\t# if video is still left continue creating images\n",
        "\t\tname = './data/frame' + str(currentframe) + '.jpg'\n",
        "\t\tprint ('Creating...' + name)\n",
        "\n",
        "\t\t# writing the extracted images\n",
        "\t\tcv2.imwrite(name, frame)\n",
        "\n",
        "\t\t# increasing counter so that it will\n",
        "\t\t# show how many frames are created\n",
        "\t\tcurrentframe += 1\n",
        "\telse:\n",
        "\t\tbreak\n",
        "\n",
        "# Release all space and windows once done\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAQlVZt5qGZW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAb3B5nsqGVs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5dBYrNiqGSx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-e4zOvMp0Rd"
      },
      "source": [
        "# Features from face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbR3hcW6lc2W",
        "outputId": "a77c3600-a3b3-40ee-9d2e-55317129ad3a"
      },
      "source": [
        "from io import BytesIO\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "\n",
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from azure.cognitiveservices.vision.face.models import FaceAttributeType\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "# This key will serve all examples in this document.\n",
        "KEY = '8484b232906249b3851bda2a5b49b70e'\n",
        "\n",
        "# This endpoint will be used in all examples in this quickstart.\n",
        "ENDPOINT = 'https://newface2021.cognitiveservices.azure.com/'\n",
        "\n",
        "# Create an authenticated FaceClient.\n",
        "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))\n",
        "\n",
        "# Image of face(s)\n",
        "face1_url = 'https://upload.wikimedia.org/wikipedia/commons/5/55/Dalailama1_20121014_4639.jpg'\n",
        "face1_name = os.path.basename(face1_url)\n",
        "# face2_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/The_famous_mustache_and_goatee.jpg/220px-The_famous_mustache_and_goatee.jpg'\n",
        "# face2_name = os.path.basename(face2_url)\n",
        "\n",
        "# List of url images\n",
        "url_images = [face1_url]\n",
        "\n",
        "# Attributes you want returned with the API call, a list of FaceAttributeType enum (string format)\n",
        "face_attributes = ['age', 'gender', 'headPose', 'smile', 'facialHair', 'glasses', 'emotion']\n",
        "\n",
        "# Detect a face with attributes, returns a list[DetectedFace]\n",
        "for image in url_images:\n",
        "    detected_faces = face_client.face.detect_with_url(url=image, return_face_attributes=face_attributes)\n",
        "    if not detected_faces:\n",
        "        raise Exception(\n",
        "            'No face detected from image {}'.format(os.path.basename(image)))\n",
        "\n",
        "    '''\n",
        "    Display the detected face with attributes and bounding box\n",
        "    '''\n",
        "    # Face IDs are used for comparison to faces (their IDs) detected in other images.\n",
        "    for face in detected_faces:\n",
        "        print()\n",
        "        print('Detected face ID from', os.path.basename(image), ':')\n",
        "        # ID of detected face\n",
        "        print(face.face_id)\n",
        "        # Show all facial attributes from the results\n",
        "        print()\n",
        "        print('Facial attributes detected:')\n",
        "        print('Age: ', face.face_attributes.age)\n",
        "        print('Gender: ', face.face_attributes.gender)\n",
        "        print('Head pose: ', face.face_attributes.head_pose)\n",
        "        print('Smile: ', face.face_attributes.smile)\n",
        "        print('Facial hair: ', face.face_attributes.facial_hair)\n",
        "        print('Glasses: ', face.face_attributes.glasses)\n",
        "        print('Emotion: ')\n",
        "        print('\\tAnger: ', face.face_attributes.emotion.anger)\n",
        "        print('\\tContempt: ', face.face_attributes.emotion.contempt)\n",
        "        print('\\tDisgust: ', face.face_attributes.emotion.disgust)\n",
        "        print('\\tFear: ', face.face_attributes.emotion.fear)\n",
        "        print('\\tHappiness: ', face.face_attributes.emotion.happiness)\n",
        "        print('\\tNeutral: ', face.face_attributes.emotion.neutral)\n",
        "        print('\\tSadness: ', face.face_attributes.emotion.sadness)\n",
        "        print('\\tSurprise: ', face.face_attributes.emotion.surprise)\n",
        "        print()\n",
        "\n",
        "    # Convert width height to a point in a rectangle\n",
        "    def getRectangle(faceDictionary):\n",
        "        rect = faceDictionary.face_rectangle\n",
        "        left = rect.left\n",
        "        top = rect.top\n",
        "        right = left + rect.width\n",
        "        bottom = top + rect.height\n",
        "\n",
        "        return ((left, top), (right, bottom))\n",
        "\n",
        "    # Download the image from the url, so can display it in popup/browser\n",
        "    response = requests.get(image)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # For each face returned use the face rectangle and draw a blue box.\n",
        "    print('Drawing rectangle around face... see popup for results.')\n",
        "    print()\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for face in detected_faces:\n",
        "        draw.rectangle(getRectangle(face), outline='blue')\n",
        "\n",
        "    # Display the image in the users default image browser.\n",
        "    img=img.save('Face.jpeg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Detected face ID from Dalailama1_20121014_4639.jpg :\n",
            "d85695ca-98fb-4461-a41f-6792a5a5babd\n",
            "\n",
            "Facial attributes detected:\n",
            "Age:  65.0\n",
            "Gender:  Gender.male\n",
            "Head pose:  {'additional_properties': {}, 'roll': -6.4, 'yaw': -4.6, 'pitch': -12.5}\n",
            "Smile:  0.993\n",
            "Facial hair:  {'additional_properties': {}, 'moustache': 0.1, 'beard': 0.1, 'sideburns': 0.1}\n",
            "Glasses:  GlassesType.no_glasses\n",
            "Emotion: \n",
            "\tAnger:  0.0\n",
            "\tContempt:  0.005\n",
            "\tDisgust:  0.0\n",
            "\tFear:  0.0\n",
            "\tHappiness:  0.993\n",
            "\tNeutral:  0.002\n",
            "\tSadness:  0.0\n",
            "\tSurprise:  0.0\n",
            "\n",
            "Drawing rectangle around face... see popup for results.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPk847FCqBUQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrYeDeG0qBIA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCOlC_I7qBEj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QhcAgmPp8Mx"
      },
      "source": [
        "# Features(Emotions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkp4XWj_lc5y",
        "outputId": "9992e317-8e9a-430f-96ec-f72b0d3af37a"
      },
      "source": [
        "from io import BytesIO\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "\n",
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from azure.cognitiveservices.vision.face.models import FaceAttributeType\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "\n",
        "# This key will serve all examples in this document.\n",
        "KEY = '8484b232906249b3851bda2a5b49b70e'\n",
        "\n",
        "# This endpoint will be used in all examples in this quickstart.\n",
        "ENDPOINT = 'https://newface2021.cognitiveservices.azure.com/'\n",
        "\n",
        "# Create an authenticated FaceClient.\n",
        "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))\n",
        "\n",
        "# Image of face(s)\n",
        "face1_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/President_Trump_Delivers_Remarks_%2849498772251%29.jpg/1280px-President_Trump_Delivers_Remarks_%2849498772251%29.jpg'\n",
        "face1_name = os.path.basename(face1_url)\n",
        "# face2_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/The_famous_mustache_and_goatee.jpg/220px-The_famous_mustache_and_goatee.jpg'\n",
        "# face2_name = os.path.basename(face2_url)\n",
        "\n",
        "# List of url images\n",
        "url_images = [face1_url]\n",
        "\n",
        "# Attributes you want returned with the API call, a list of FaceAttributeType enum (string format)\n",
        "face_attributes = ['age', 'gender', 'glasses', 'emotion']\n",
        "\n",
        "# Detect a face with attributes, returns a list[DetectedFace]\n",
        "for image in url_images:\n",
        "    detected_faces = face_client.face.detect_with_url(url=image, return_face_attributes=face_attributes)\n",
        "    if not detected_faces:\n",
        "        raise Exception(\n",
        "            'No face detected from image {}'.format(os.path.basename(image)))\n",
        "\n",
        "    '''\n",
        "    Display the detected face with attributes and bounding box\n",
        "    '''\n",
        "    # Face IDs are used for comparison to faces (their IDs) detected in other images.\n",
        "    for face in detected_faces:\n",
        "        print()\n",
        "        print('Detected face ID from', os.path.basename(image), ':')\n",
        "        # ID of detected face\n",
        "        print(face.face_id)\n",
        "        # Show all facial attributes from the results\n",
        "        print()\n",
        "        print('Facial attributes detected:')\n",
        "        print('Age: ', face.face_attributes.age)\n",
        "        print('Gender: ', face.face_attributes.gender)\n",
        "        print('Glasses: ', face.face_attributes.glasses)\n",
        "        print('Emotion: ')\n",
        "        print('\\tAnger: ', face.face_attributes.emotion.anger)\n",
        "        print('\\tContempt: ', face.face_attributes.emotion.contempt)\n",
        "        print('\\tDisgust: ', face.face_attributes.emotion.disgust)\n",
        "        print('\\tFear: ', face.face_attributes.emotion.fear)\n",
        "        print('\\tHappiness: ', face.face_attributes.emotion.happiness)\n",
        "        print('\\tNeutral: ', face.face_attributes.emotion.neutral)\n",
        "        print('\\tSadness: ', face.face_attributes.emotion.sadness)\n",
        "        print('\\tSurprise: ', face.face_attributes.emotion.surprise)\n",
        "        print()\n",
        "\n",
        "    # Convert width height to a point in a rectangle\n",
        "    def getRectangle(faceDictionary):\n",
        "        rect = faceDictionary.face_rectangle\n",
        "        left = rect.left\n",
        "        top = rect.top\n",
        "        right = left + rect.width\n",
        "        bottom = top + rect.height\n",
        "\n",
        "        return ((left, top), (right, bottom))\n",
        "\n",
        "    # Download the image from the url, so can display it in popup/browser\n",
        "    response = requests.get(image)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # For each face returned use the face rectangle and draw a blue box.\n",
        "    print('Drawing rectangle around face... see popup for results.')\n",
        "    print()\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for face in detected_faces:\n",
        "        draw.rectangle(getRectangle(face), outline='blue')\n",
        "\n",
        "    # Display the image in the users default image browser.\n",
        "    img=img.save('Face1.jpeg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Detected face ID from 1280px-President_Trump_Delivers_Remarks_%2849498772251%29.jpg :\n",
            "94d24dc4-580f-4a96-8233-d15c35950aac\n",
            "\n",
            "Facial attributes detected:\n",
            "Age:  60.0\n",
            "Gender:  Gender.male\n",
            "Glasses:  GlassesType.no_glasses\n",
            "Emotion: \n",
            "\tAnger:  0.0\n",
            "\tContempt:  0.004\n",
            "\tDisgust:  0.0\n",
            "\tFear:  0.0\n",
            "\tHappiness:  0.007\n",
            "\tNeutral:  0.828\n",
            "\tSadness:  0.161\n",
            "\tSurprise:  0.0\n",
            "\n",
            "Drawing rectangle around face... see popup for results.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}